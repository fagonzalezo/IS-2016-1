{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz 2\n",
    "# Intelligent Systems 2016-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After solving all the questions in the exam save your notebook with the name `username.ipynb` and submit it to: https://www.dropbox.com/request/0Eh9d2PvQMdAyJviK4Nl\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mdp import *\n",
    "from rl import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. (1.7)\n",
    "Implement a MDP that solves the question 2 of HW4 from [AI-edX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LinearMDP(GridMDP):\n",
    "    \"\"\"A two-dimensional grid MDP, as in [Figure 17.1].  All you have to do is\n",
    "    specify the grid as a list of lists of rewards; use None for an obstacle\n",
    "    (unreachable state).  Also, you should specify the terminal states.\n",
    "    An action is an (x, y) unit vector; e.g. (1, 0) means move east.\"\"\"\n",
    "\n",
    "    def __init__(self, grid, terminals, init=(0, 0), gamma=.9):\n",
    "        GridMDP.__init__(self, grid=grid, terminals=terminals, init=init, gamma=gamma)\n",
    "\n",
    "    def T(self, state, action):\n",
    "        \"\"\"\n",
    "        This function must return a list of tuples (p, state') where p is the probability \n",
    "        of going to state'\n",
    "        \"\"\"\n",
    "        # Your code here #\n",
    "\n",
    "def calculate_v_star(rew_a, gamma):\n",
    "    '''\n",
    "    This function must create an instance of LinearMDP that corresponds to the \n",
    "    MDP in question 2 of HW4 and use it to calculate the expected reward for each state.\n",
    "    The function receives as parameter the reward for state 'a', which in the example\n",
    "    corresponds to 10, but here is variable, and the value of gamma. \n",
    "    The reward for state 'e' is 1. The function must return a dictionary with the V* values. \n",
    "    '''\n",
    "    # Your code here #\n",
    "    mdp = \n",
    "    v_star =\n",
    "    ##################\n",
    "    return v_star"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. (1.7)\n",
    "Implement a MDP corresponding to the one in question 3 of HW4 from [AI-edX]. Modify the value iteration algorithm to consider rewards that depend on $(s, a, s')$, where $s$, $a$, and $s'$ correspond to the current state, the action and the next state respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ExtendedMDP(MDP):\n",
    "\n",
    "    def __init__(self, transition_matrix, rewards, terminals, init, gamma=.9):\n",
    "        # All possible actions.\n",
    "        actlist = []\n",
    "        for state in transition_matrix.keys():\n",
    "            actlist.extend(transition_matrix[state])\n",
    "        actlist = list(set(actlist))\n",
    "\n",
    "        MDP.__init__(self, init, actlist, terminals=terminals, gamma=gamma)\n",
    "        self.t = transition_matrix\n",
    "        self.reward = rewards\n",
    "        for state in self.t:\n",
    "            self.states.add(state)\n",
    "\n",
    "    def T(self, state, action):\n",
    "        return [(prob, new_state) for new_state, prob \n",
    "                in self.t[state][action].items()]\n",
    "\n",
    "    def R(self, state, action, statep):\n",
    "        \"Returns a numeric reward for the combination the current state, the action and the next state.\"\n",
    "        return self.reward[state][action][statep]\n",
    "    \n",
    "def ext_value_iteration(mdp, epsilon=0.001):\n",
    "    \"Solving an MDP by value iteration. Uses a reward function that depends on s, a and s1 \"\n",
    "    U1 = {s: 0 for s in mdp.states}\n",
    "    R, T, gamma = mdp.R, mdp.T, mdp.gamma\n",
    "    while True:\n",
    "        U = U1.copy()\n",
    "        delta = 0\n",
    "        for s in mdp.states:\n",
    "            # Your code here #\n",
    "            U1[s] = \n",
    "            ##################\n",
    "            delta = max(delta, abs(U1[s] - U[s]))\n",
    "        if delta < epsilon * (1 - gamma) / gamma:\n",
    "            return U\n",
    "        \n",
    "def ext_calculate_v_star(gamma):\n",
    "    '''\n",
    "    This function must create an instance of ExtendedMDP that corresponds to the \n",
    "    MDP in question 3 of HW4 and use it to calculate the expected reward for each state.\n",
    "    The function receives as parameter the value of gamma. \n",
    "    The function must return a dictionary with the V* values. \n",
    "    '''\n",
    "    # Your code here #\n",
    "    t = \n",
    "    rewards = \n",
    "    ##################\n",
    "    emdp = ExtendedMDP(t, rewards, [], None, gamma=gamma)\n",
    "    v_star = ext_value_iteration(emdp, epsilon = 0.00001)\n",
    "    return v_star"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. (1.6)\n",
    "Apply Q-learning to calculate an optimal policy for the LinearMDP in question 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calculate_policy(rew_a, gamma):\n",
    "    '''\n",
    "    This function must create an instance of LinearMDP that corresponds to the \n",
    "    MDP in question 2 of HW4 and use it to calculate an optimal policy by applying\n",
    "    Q-learning. The function receives as parameter the reward for state 'a', \n",
    "    which in the example corresponds to 10, but here is variable, and the value of gamma. \n",
    "    The reward for state 'e' is 1. The function must return a dictionary with an action for\n",
    "    each state.\n",
    "    '''\n",
    "    # Your code here #\n",
    "    mdp = \n",
    "    ##################\n",
    "    q_agent = QLearningAgent(mdp, Ne=5, Rplus=10, \n",
    "                         alpha=lambda n: 60./(59+n))\n",
    "    for i in range(200):\n",
    "        run_single_trial(q_agent,mdp)\n",
    "    U = defaultdict(lambda: -1000.) # Very Large Negative Value for Comparison see below.\n",
    "    policy = {}\n",
    "    # Your code here #\n",
    "    ##################\n",
    "    return policy\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
